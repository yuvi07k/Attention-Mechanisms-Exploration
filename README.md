# Attention-Mechanisms-Exploration

Transformers have become foundational in Natural Language Processing (NLP), largely due to their at- tention mechanisms, particularly multi-headed atten- tion. While multi-headed attention enables models to represent various linguistic features and dependencies in parallel, it remains unclear how critical each indi- vidual head is to downstream performance, and what impact reducing the model to a single-head attention configuration would have. This study examines these questions through controlled experiments with two popular pre-trained transformer-based models, GPT- 2 (an autoregressive decoder-only model) and Distil- BERT (a compact encoder-only model). I measure the importance of individual attention heads by analyzing gradient-based metrics and then selectively disable heads identified as most or least important, as well as random heads, followed by further fine-tuning.
Results on the Stanford Sentiment Treebank (SST-2) binary classification task show that disabling a small number of the least important heads can often be done with minimal performance degradation, and in some cases (for GPT-2) even slightly improve final accuracy after re-fine-tuning. Conversely, disabling the most important heads results in consistent performance drops. Further, I examine models that use only a single attention head per layer. While single-headed variants reach moderate accuracy levels, they generally un- derperform their multi-headed counterparts and show limited improvement with extended training.
These findings highlight that while certain attention heads are crucial for maintaining high accuracy, oth- ers are redundant or even potentially detrimental. I also observe that multi-headed attention is particularly beneficial, and reducing to a single head may limit representational capacity. My results inform model interpretability and efficiency research, suggesting that targeted head pruning or adjustments can serve as an effective technique for reducing complexity.

