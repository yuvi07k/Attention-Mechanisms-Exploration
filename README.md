# Multi-Headed vs. Single-Headed Attention in Transformer-based Models


